{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sven/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/sven/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/sven/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/sven/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sven/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sven/nltk_data', '/opt/anaconda3/nltk_data', '/opt/anaconda3/share/nltk_data', '/opt/anaconda3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_file = \"training_data_lowercase.csv\"\n",
    "data = pd.read_csv(data_file, sep=\"\\t\", header=None)  # Load as tab-delimited file without headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>donald trump sends out embarrassing new year‚s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>sheriff david clarke becomes an internet joke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>trump is so obsessed he even has obama‚s name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  donald trump sends out embarrassing new year‚s...\n",
       "1      0  drunk bragging trump staffer started russian c...\n",
       "2      0  sheriff david clarke becomes an internet joke ...\n",
       "3      0  trump is so obsessed he even has obama‚s name ...\n",
       "4      0  pope francis just called out donald trump duri..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign new column names\n",
    "data.columns = ['label', 'text']\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "data['text'] = data['text'].str.replace(r'[^a-z0-9\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and remove stopwords for each text entry in the dataframe\n",
    "data['filtered_text'] = data['text'].apply(lambda x: [word for word in word_tokenize(x) if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text  \\\n",
      "0      0  donald trump sends out embarrassing new years ...   \n",
      "1      0  drunk bragging trump staffer started russian c...   \n",
      "2      0  sheriff david clarke becomes an internet joke ...   \n",
      "3      0  trump is so obsessed he even has obamas name c...   \n",
      "4      0  pope francis just called out donald trump duri...   \n",
      "\n",
      "                                       filtered_text  \\\n",
      "0  [donald, trump, sends, embarrassing, new, year...   \n",
      "1  [drunk, bragging, trump, staffer, started, rus...   \n",
      "2  [sheriff, david, clarke, becomes, internet, jo...   \n",
      "3  [trump, obsessed, even, obamas, name, coded, w...   \n",
      "4  [pope, francis, called, donald, trump, christm...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [donald, trump, sends, embarrass, new, year, e...  \n",
      "1  [drunk, bragging, trump, staffer, start, russi...  \n",
      "2  [sheriff, david, clarke, becomes, internet, jo...  \n",
      "3  [trump, obsess, even, obamas, name, cod, websi...  \n",
      "4  [pope, francis, call, donald, trump, christmas...  \n"
     ]
    }
   ],
   "source": [
    "# Lemmatization of text to leverage the context as well\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get part-of-speech (POS) tagging for more accurate lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,  # Adjective\n",
    "        'N': wordnet.NOUN,  # Noun\n",
    "        'V': wordnet.VERB,  # Verb\n",
    "        'R': wordnet.ADV   # Adverb\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun\n",
    "\n",
    "# Apply lemmatization to the filtered_text column\n",
    "data['lemmatized_text'] = data['filtered_text'].apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the result\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[1;32m      8\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# Limit to top 5000 words \u001b[39;00m\n\u001b[1;32m      9\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Exclude common stopwords \u001b[39;00m\n\u001b[1;32m     10\u001b[0m     ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Consider unigrams and bigrams \u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Fit and transform the lemmatized text column\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Convert the resulting sparse matrix to a DataFrame for analysis\u001b[39;00m\n\u001b[1;32m     17\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2090\u001b[0m )\n\u001b[0;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1280\u001b[0m         )\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# TF IDF\n",
    "\n",
    "# Combine tokens into single text strings for each row in 'lemmatized_text'\n",
    "data['lemmatized_text'] = data['lemmatized_text'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Limit to top 5000 words \n",
    "    stop_words='english',  # Exclude common stopwords \n",
    "    ngram_range=(1, 2)  # Consider unigrams and bigrams \n",
    ")\n",
    "\n",
    "# Fit and transform the lemmatized text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "# Convert the resulting sparse matrix to a DataFrame for analysis\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10  10 year  100  100 day  1000   11   12   13   14   15  ...  zika  \\\n",
      "0   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "1   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "2   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "3   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "4   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "5   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "6   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "7   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "8   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "9   0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "10  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "11  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "12  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "13  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "14  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "15  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "16  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "17  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "18  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "19  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "20  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "21  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "22  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "23  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "24  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "25  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "26  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "27  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "28  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "29  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "30  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "31  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "32  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "33  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "34  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "35  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "36  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "37  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "38  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "39  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "40  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "41  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "42  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "43  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "44  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "45  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "46  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "47  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "48  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "49  0.0      0.0  0.0      0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
      "\n",
      "    zika funding  zika virus  zimbabwe  zimbabwe mnangagwa  zimbabwe mugabe  \\\n",
      "0            0.0         0.0       0.0                 0.0              0.0   \n",
      "1            0.0         0.0       0.0                 0.0              0.0   \n",
      "2            0.0         0.0       0.0                 0.0              0.0   \n",
      "3            0.0         0.0       0.0                 0.0              0.0   \n",
      "4            0.0         0.0       0.0                 0.0              0.0   \n",
      "5            0.0         0.0       0.0                 0.0              0.0   \n",
      "6            0.0         0.0       0.0                 0.0              0.0   \n",
      "7            0.0         0.0       0.0                 0.0              0.0   \n",
      "8            0.0         0.0       0.0                 0.0              0.0   \n",
      "9            0.0         0.0       0.0                 0.0              0.0   \n",
      "10           0.0         0.0       0.0                 0.0              0.0   \n",
      "11           0.0         0.0       0.0                 0.0              0.0   \n",
      "12           0.0         0.0       0.0                 0.0              0.0   \n",
      "13           0.0         0.0       0.0                 0.0              0.0   \n",
      "14           0.0         0.0       0.0                 0.0              0.0   \n",
      "15           0.0         0.0       0.0                 0.0              0.0   \n",
      "16           0.0         0.0       0.0                 0.0              0.0   \n",
      "17           0.0         0.0       0.0                 0.0              0.0   \n",
      "18           0.0         0.0       0.0                 0.0              0.0   \n",
      "19           0.0         0.0       0.0                 0.0              0.0   \n",
      "20           0.0         0.0       0.0                 0.0              0.0   \n",
      "21           0.0         0.0       0.0                 0.0              0.0   \n",
      "22           0.0         0.0       0.0                 0.0              0.0   \n",
      "23           0.0         0.0       0.0                 0.0              0.0   \n",
      "24           0.0         0.0       0.0                 0.0              0.0   \n",
      "25           0.0         0.0       0.0                 0.0              0.0   \n",
      "26           0.0         0.0       0.0                 0.0              0.0   \n",
      "27           0.0         0.0       0.0                 0.0              0.0   \n",
      "28           0.0         0.0       0.0                 0.0              0.0   \n",
      "29           0.0         0.0       0.0                 0.0              0.0   \n",
      "30           0.0         0.0       0.0                 0.0              0.0   \n",
      "31           0.0         0.0       0.0                 0.0              0.0   \n",
      "32           0.0         0.0       0.0                 0.0              0.0   \n",
      "33           0.0         0.0       0.0                 0.0              0.0   \n",
      "34           0.0         0.0       0.0                 0.0              0.0   \n",
      "35           0.0         0.0       0.0                 0.0              0.0   \n",
      "36           0.0         0.0       0.0                 0.0              0.0   \n",
      "37           0.0         0.0       0.0                 0.0              0.0   \n",
      "38           0.0         0.0       0.0                 0.0              0.0   \n",
      "39           0.0         0.0       0.0                 0.0              0.0   \n",
      "40           0.0         0.0       0.0                 0.0              0.0   \n",
      "41           0.0         0.0       0.0                 0.0              0.0   \n",
      "42           0.0         0.0       0.0                 0.0              0.0   \n",
      "43           0.0         0.0       0.0                 0.0              0.0   \n",
      "44           0.0         0.0       0.0                 0.0              0.0   \n",
      "45           0.0         0.0       0.0                 0.0              0.0   \n",
      "46           0.0         0.0       0.0                 0.0              0.0   \n",
      "47           0.0         0.0       0.0                 0.0              0.0   \n",
      "48           0.0         0.0       0.0                 0.0              0.0   \n",
      "49           0.0         0.0       0.0                 0.0              0.0   \n",
      "\n",
      "    zimbabwe ruling  zone  zuckerberg  zuma  \n",
      "0               0.0   0.0         0.0   0.0  \n",
      "1               0.0   0.0         0.0   0.0  \n",
      "2               0.0   0.0         0.0   0.0  \n",
      "3               0.0   0.0         0.0   0.0  \n",
      "4               0.0   0.0         0.0   0.0  \n",
      "5               0.0   0.0         0.0   0.0  \n",
      "6               0.0   0.0         0.0   0.0  \n",
      "7               0.0   0.0         0.0   0.0  \n",
      "8               0.0   0.0         0.0   0.0  \n",
      "9               0.0   0.0         0.0   0.0  \n",
      "10              0.0   0.0         0.0   0.0  \n",
      "11              0.0   0.0         0.0   0.0  \n",
      "12              0.0   0.0         0.0   0.0  \n",
      "13              0.0   0.0         0.0   0.0  \n",
      "14              0.0   0.0         0.0   0.0  \n",
      "15              0.0   0.0         0.0   0.0  \n",
      "16              0.0   0.0         0.0   0.0  \n",
      "17              0.0   0.0         0.0   0.0  \n",
      "18              0.0   0.0         0.0   0.0  \n",
      "19              0.0   0.0         0.0   0.0  \n",
      "20              0.0   0.0         0.0   0.0  \n",
      "21              0.0   0.0         0.0   0.0  \n",
      "22              0.0   0.0         0.0   0.0  \n",
      "23              0.0   0.0         0.0   0.0  \n",
      "24              0.0   0.0         0.0   0.0  \n",
      "25              0.0   0.0         0.0   0.0  \n",
      "26              0.0   0.0         0.0   0.0  \n",
      "27              0.0   0.0         0.0   0.0  \n",
      "28              0.0   0.0         0.0   0.0  \n",
      "29              0.0   0.0         0.0   0.0  \n",
      "30              0.0   0.0         0.0   0.0  \n",
      "31              0.0   0.0         0.0   0.0  \n",
      "32              0.0   0.0         0.0   0.0  \n",
      "33              0.0   0.0         0.0   0.0  \n",
      "34              0.0   0.0         0.0   0.0  \n",
      "35              0.0   0.0         0.0   0.0  \n",
      "36              0.0   0.0         0.0   0.0  \n",
      "37              0.0   0.0         0.0   0.0  \n",
      "38              0.0   0.0         0.0   0.0  \n",
      "39              0.0   0.0         0.0   0.0  \n",
      "40              0.0   0.0         0.0   0.0  \n",
      "41              0.0   0.0         0.0   0.0  \n",
      "42              0.0   0.0         0.0   0.0  \n",
      "43              0.0   0.0         0.0   0.0  \n",
      "44              0.0   0.0         0.0   0.0  \n",
      "45              0.0   0.0         0.0   0.0  \n",
      "46              0.0   0.0         0.0   0.0  \n",
      "47              0.0   0.0         0.0   0.0  \n",
      "48              0.0   0.0         0.0   0.0  \n",
      "49              0.0   0.0         0.0   0.0  \n",
      "\n",
      "[50 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the TF-IDF DataFrame\n",
    "print(tfidf_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    d   o   n   a   l   d       t   r   u   m   p ...\n",
      "1    d   r   u   n   k       b   r   a   g   g   i ...\n",
      "2    s   h   e   r   i   f   f       d   a   v   i ...\n",
      "3    t   r   u   m   p       o   b   s   e   s   s ...\n",
      "4    p   o   p   e       f   r   a   n   c   i   s ...\n",
      "5    r   a   c   i   s   t       a   l   a   b   a ...\n",
      "6    f   r   e   s   h       g   o   l   f       c ...\n",
      "7    t   r   u   m   p       s   a   y       i   n ...\n",
      "8    f   o   r   m   e   r       c   i   a       d ...\n",
      "9    b   r   a   n   d   n   e   w       p   r   o ...\n",
      "Name: lemmatized_text, dtype: object\n",
      "0\n",
      "count    34152.000000\n",
      "mean       233.234950\n",
      "std         70.746604\n",
      "min          0.000000\n",
      "25%        193.000000\n",
      "50%        225.000000\n",
      "75%        265.000000\n",
      "max        913.000000\n",
      "Name: lemmatized_text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['lemmatized_text'].head(10))\n",
    "print(data['lemmatized_text'].isna().sum())  # Check for NaN values\n",
    "print(data['lemmatized_text'].apply(len).describe())  # Analyze lengths of text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
