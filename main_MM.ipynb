{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mehak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\mehak/nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\ProgramData\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\mehak\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (update file path accordingly)\n",
    "data_file = \"training_data_lowercase.csv\"\n",
    "data = pd.read_csv(data_file, sep=\"\\t\", header=None)  # Tab-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>donald trump sends out embarrassing new year‚s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>drunk bragging trump staffer started russian c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>sheriff david clarke becomes an internet joke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>trump is so obsessed he even has obama‚s name ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>pope francis just called out donald trump duri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  donald trump sends out embarrassing new year‚s...\n",
       "1      0  drunk bragging trump staffer started russian c...\n",
       "2      0  sheriff david clarke becomes an internet joke ...\n",
       "3      0  trump is so obsessed he even has obama‚s name ...\n",
       "4      0  pope francis just called out donald trump duri..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['label', 'text'] \n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and remove stopwords\n",
    "data['filtered_text'] = data['text'].apply(lambda x: [word for word in word_tokenize(x.lower()) if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text  \\\n",
      "0      0  donald trump sends out embarrassing new years ...   \n",
      "1      0  drunk bragging trump staffer started russian c...   \n",
      "2      0  sheriff david clarke becomes an internet joke ...   \n",
      "3      0  trump is so obsessed he even has obamas name c...   \n",
      "4      0  pope francis just called out donald trump duri...   \n",
      "\n",
      "                                       filtered_text  \\\n",
      "0  [donald, trump, sends, embarrassing, new, year...   \n",
      "1  [drunk, bragging, trump, staffer, started, rus...   \n",
      "2  [sheriff, david, clarke, becomes, internet, jo...   \n",
      "3  [trump, obsessed, even, obamas, name, coded, w...   \n",
      "4  [pope, francis, called, donald, trump, christm...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  [donald, trump, sends, embarrass, new, year, e...  \n",
      "1  [drunk, bragging, trump, staffer, start, russi...  \n",
      "2  [sheriff, david, clarke, becomes, internet, jo...  \n",
      "3  [trump, obsess, even, obamas, name, cod, websi...  \n",
      "4  [pope, francis, call, donald, trump, christmas...  \n"
     ]
    }
   ],
   "source": [
    "# Lemmatization of text to leverage the context as well\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get part-of-speech (POS) tagging for more accurate lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk import pos_tag\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,  # Adjective\n",
    "        'N': wordnet.NOUN,  # Noun\n",
    "        'V': wordnet.VERB,  # Verb\n",
    "        'R': wordnet.ADV   # Adverb\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun\n",
    "\n",
    "# Apply lemmatization to the filtered_text column\n",
    "data['lemmatized_text'] = data['filtered_text'].apply(\n",
    "    lambda tokens: [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the result\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tokens into single text strings for each row in 'lemmatized_text'\n",
    "data['lemmatized_text'] = data['lemmatized_text'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        donald trump sends embarrass new year eve mess...\n",
      "1        drunk bragging trump staffer start russian col...\n",
      "2        sheriff david clarke becomes internet joke thr...\n",
      "3          trump obsess even obamas name cod website image\n",
      "4          pope francis call donald trump christmas speech\n",
      "                               ...                        \n",
      "34147              tear rain thai gather late king funeral\n",
      "34148    pyongyang university need nonus teacher travel...\n",
      "34149    philippine president duterte visit japan ahead...\n",
      "34150             japan abe may election many dont want pm\n",
      "34151      demoralize divide inside catalonia police force\n",
      "Name: lemmatized_text, Length: 34152, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,  # Adjust as needed\n",
    "    stop_words='english',  # Ignore common stopwords\n",
    "    ngram_range=(1, 2)  # Unigrams and bigrams\n",
    ")\n",
    "\n",
    "print(data['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        donald trump sends embarrass new year eve mess...\n",
      "1        drunk bragging trump staffer start russian col...\n",
      "2        sheriff david clarke becomes internet joke thr...\n",
      "3          trump obsess even obamas name cod website image\n",
      "4          pope francis call donald trump christmas speech\n",
      "                               ...                        \n",
      "34147              tear rain thai gather late king funeral\n",
      "34148    pyongyang university need nonus teacher travel...\n",
      "34149    philippine president duterte visit japan ahead...\n",
      "34150             japan abe may election many dont want pm\n",
      "34151      demoralize divide inside catalonia police force\n",
      "Name: lemmatized_text, Length: 34152, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the lemmatized text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "print(data['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the resulting sparse matrix to a DataFrame for analysis\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), \n",
    "    columns=tfidf_vectorizer.get_feature_names_out() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    america  american    attack  ban     black    break  campaign  chief  \\\n",
      "0       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "1       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "2       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "3       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "4       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "5       0.0       0.0  0.000000  0.0  0.677261  0.00000  0.000000    0.0   \n",
      "6       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "7       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "8       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "9       0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "10      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "11      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "12      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "13      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "14      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "15      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "16      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "17      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "18      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "19      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "20      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "21      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "22      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "23      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "24      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "25      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.569852    0.0   \n",
      "26      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "27      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "28      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "29      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "30      0.0       0.0  0.000000  0.0  0.000000  1.00000  0.000000    0.0   \n",
      "31      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "32      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "33      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "34      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "35      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "36      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "37      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "38      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "39      0.0       0.0  0.475528  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "40      0.0       0.0  0.480705  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "41      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "42      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "43      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "44      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "45      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "46      0.0       0.0  0.000000  0.0  0.000000  0.91792  0.000000    0.0   \n",
      "47      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "48      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.550505    0.0   \n",
      "49      0.0       0.0  0.000000  0.0  0.000000  0.00000  0.000000    0.0   \n",
      "\n",
      "    china  clinton  ...     video  vote     voter      want     watch  \\\n",
      "0     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "1     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "2     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "3     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "4     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "5     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "6     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "7     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "8     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "9     0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "10    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "11    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "12    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "13    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "14    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "15    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "16    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "17    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "18    0.0      0.0  ...  0.519227   0.0  0.000000  0.000000  0.000000   \n",
      "19    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "20    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "21    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "22    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "23    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "24    0.0      0.0  ...  0.347956   0.0  0.000000  0.000000  0.000000   \n",
      "25    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "26    0.0      0.0  ...  0.519227   0.0  0.000000  0.000000  0.000000   \n",
      "27    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "28    0.0      0.0  ...  0.427645   0.0  0.000000  0.000000  0.000000   \n",
      "29    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "30    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "31    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "32    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "33    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "34    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "35    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "36    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "37    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "38    0.0      0.0  ...  0.000000   0.0  0.000000  0.685956  0.000000   \n",
      "39    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "40    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "41    0.0      0.0  ...  0.810502   0.0  0.000000  0.000000  0.000000   \n",
      "42    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "43    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "44    0.0      0.0  ...  0.377524   0.0  0.000000  0.000000  0.676932   \n",
      "45    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "46    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "47    0.0      0.0  ...  0.000000   0.0  0.000000  0.000000  0.000000   \n",
      "48    0.0      0.0  ...  0.000000   0.0  0.615841  0.000000  0.000000   \n",
      "49    0.0      0.0  ...  0.810502   0.0  0.000000  0.000000  0.000000   \n",
      "\n",
      "       white  white house       win  woman      year  \n",
      "0   0.000000     0.000000  0.000000    0.0  0.553761  \n",
      "1   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "2   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "3   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "4   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "5   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "6   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "7   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "8   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "9   0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "10  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "11  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "12  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "13  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "14  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "15  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "16  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "17  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "18  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "19  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "20  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "21  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "22  0.000000     0.000000  0.745675    0.0  0.000000  \n",
      "23  0.000000     0.000000  0.000000    0.0  0.477224  \n",
      "24  0.519720     0.556318  0.000000    0.0  0.000000  \n",
      "25  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "26  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "27  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "28  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "29  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "30  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "31  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "32  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "33  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "34  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "35  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "36  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "37  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "38  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "39  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "40  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "41  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "42  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "43  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "44  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "45  0.575448     0.615971  0.000000    0.0  0.000000  \n",
      "46  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "47  0.900170     0.000000  0.000000    0.0  0.000000  \n",
      "48  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "49  0.000000     0.000000  0.000000    0.0  0.000000  \n",
      "\n",
      "[50 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the TF-IDF DataFrame\n",
    "print(tfidf_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    donald trump sends embarrass new year eve mess...\n",
      "1    drunk bragging trump staffer start russian col...\n",
      "2    sheriff david clarke becomes internet joke thr...\n",
      "3      trump obsess even obamas name cod website image\n",
      "4      pope francis call donald trump christmas speech\n",
      "5    racist alabama cop brutalize black boy handcuf...\n",
      "6                                    fresh golf course\n",
      "7    trump say insanely racist stuff inside oval of...\n",
      "8           former cia director slam trump un bullying\n",
      "9     brandnew protrump ad feature much kiss make sick\n",
      "Name: lemmatized_text, dtype: object\n",
      "0\n",
      "count    34152.000000\n",
      "mean        59.058562\n",
      "std         17.687234\n",
      "min          0.000000\n",
      "25%         49.000000\n",
      "50%         57.000000\n",
      "75%         67.000000\n",
      "max        229.000000\n",
      "Name: lemmatized_text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['lemmatized_text'].head(10))\n",
    "print(data['lemmatized_text'].isna().sum())  # Check for NaN values\n",
    "print(data['lemmatized_text'].apply(len).describe())  # Analyze lengths of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_df  # Feature set\n",
    "y = data['label']  # Labels\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8003220611916264\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80      3529\n",
      "           1       0.77      0.83      0.80      3302\n",
      "\n",
      "    accuracy                           0.80      6831\n",
      "   macro avg       0.80      0.80      0.80      6831\n",
      "weighted avg       0.80      0.80      0.80      6831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8028107158541942\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.74      0.80      3529\n",
      "           1       0.76      0.87      0.81      3302\n",
      "\n",
      "    accuracy                           0.80      6831\n",
      "   macro avg       0.81      0.80      0.80      6831\n",
      "weighted avg       0.81      0.80      0.80      6831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree Classifier\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NaN values:\", data['lemmatized_text'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    34152.000000\n",
      "mean        59.058562\n",
      "std         17.687234\n",
      "min          0.000000\n",
      "25%         49.000000\n",
      "50%         57.000000\n",
      "75%         67.000000\n",
      "max        229.000000\n",
      "Name: lemmatized_text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['lemmatized_text'].apply(len).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Features:\n",
      "    america  american  attack  ban  black  break  campaign  chief  china  \\\n",
      "0      0.0       0.0     0.0  0.0    0.0    0.0       0.0    0.0    0.0   \n",
      "1      0.0       0.0     0.0  0.0    0.0    0.0       0.0    0.0    0.0   \n",
      "2      0.0       0.0     0.0  0.0    0.0    0.0       0.0    0.0    0.0   \n",
      "3      0.0       0.0     0.0  0.0    0.0    0.0       0.0    0.0    0.0   \n",
      "4      0.0       0.0     0.0  0.0    0.0    0.0       0.0    0.0    0.0   \n",
      "\n",
      "   clinton  ...  video  vote  voter  want  watch  white  white house  win  \\\n",
      "0      0.0  ...    0.0   0.0    0.0   0.0    0.0    0.0          0.0  0.0   \n",
      "1      0.0  ...    0.0   0.0    0.0   0.0    0.0    0.0          0.0  0.0   \n",
      "2      0.0  ...    0.0   0.0    0.0   0.0    0.0    0.0          0.0  0.0   \n",
      "3      0.0  ...    0.0   0.0    0.0   0.0    0.0    0.0          0.0  0.0   \n",
      "4      0.0  ...    0.0   0.0    0.0   0.0    0.0    0.0          0.0  0.0   \n",
      "\n",
      "   woman      year  \n",
      "0    0.0  0.553761  \n",
      "1    0.0  0.000000  \n",
      "2    0.0  0.000000  \n",
      "3    0.0  0.000000  \n",
      "4    0.0  0.000000  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Features:\\n\", tfidf_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
